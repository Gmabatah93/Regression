# Regression

### Process
1. Pick & Run the Regression Method that best suits the Problem
  + _[OLS | Weighted | Total | Non-Linear | Robust | GLM]_
2. Plot & Study Residuals
  + Outliers & Influential Points
  + Normality & Homoscedasicity
  + Model Error
  + Residual Independence

## Ordinary Least Squares
"Provides the **B**est **L**inear **U**nbiased **E**stimate of the parameters if the _ASSUMPTIONS_ of OLS are True"

### Assumptions
1. E is a Random Variable that does not depend on x
2. All Ei are Independent of each other _Independent & Identically Distributed_
3. All Ei have the same probability Density Function, thus same variance _HOMOSCEDASTIC_
4. E ~ N(): _Normally Distributed_
---
## Tests

### Normality of Residuals
> **Detections**: Histogram / QQ / Shapiro-Wilk Test

### Outliers
**Outliers**: generated by a different mechanism\
**Spurious**: nothing to teach us about the subject matter of Interest

> **Detections**: Skewness/ Kurtosis / IQR Test / Grubbs Test \
> **Leverage**: Data with an extreme value of the predicted x's "Accounting for Correlation". _(Williams Graph | Hat Values)_ \
> **Influence**: Removing data point substantially changes the Regression Results. _(Cook's Distance)_

### Model Error
Some of the Variance in **y** isn't properly Explained _"Underfitting/Overfitting"_

> **Detections**: F-Test | Residual Plot | Residual Independence | Goodness of Fit | Multi-Collinearity  

### Heteroscedasticity
> **Detections**: Residual Plot | Bartlett Test

**Causes**: Wrong Model | Outliers | Non-Normal Errors
**Remedies**: Transformation(Log|Sqrt|BoxCox) | Weighted Regression 
